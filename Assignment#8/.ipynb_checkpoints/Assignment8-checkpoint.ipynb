{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train data and Add noises to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class numpyDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "def activateNoises(imagedata):\n",
    "    \n",
    "    noises = [0.01, 0.02, 0.03, 0.04]\n",
    "    \n",
    "    for img in range(len(imagedata)):\n",
    "        random_idx = np.random.randint(0,4)\n",
    "        imagedata[img] = imagedata[img] + np.random.normal(0, noises[random_idx],imagedata[img].shape)\n",
    "    \n",
    "    \n",
    "    imagedata = np.minimum(imagedata, 1)\n",
    "    imagedata = np.maximum(imagedata, 0)\n",
    "    \n",
    "    return imagedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform       = transforms.Compose([\n",
    "                                transforms.ToPILImage(),\n",
    "                                transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.ToTensor(),\n",
    "                            ])\n",
    "\n",
    "# for training\n",
    "traindata       = np.load('data/train.npy')\n",
    "noisedata       = np.load('data/train.npy')\n",
    "noisedata       = activateNoises(noisedata)\n",
    "\n",
    "valdata         = traindata[4000:,:,:]\n",
    "traindata       = traindata[:4000,:,:]\n",
    "\n",
    "noise_valdata   = noisedata[4000:,:,:]\n",
    "noisedata       = noisedata[:4000,:,:]\n",
    "\n",
    "\n",
    "traindataset    = numpyDataset(traindata, transform)\n",
    "noisedataset    = numpyDataset(noisedata, transform)\n",
    "valdataset      = numpyDataset(valdata, transform)\n",
    "noise_valdataset= numpyDataset(noise_valdata, transform)\n",
    "\n",
    "\n",
    "trainloader     = DataLoader(traindataset, batch_size=200, shuffle=False, num_workers=0)\n",
    "noiseloader     = DataLoader(noisedataset, batch_size=200, shuffle=False, num_workers=0)\n",
    "valloader       = DataLoader(valdataset, batch_size=100, shuffle=False, num_workers=0)\n",
    "noise_valloader = DataLoader(noise_valdataset, batch_size=100, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(16, 8, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(8)\n",
    "            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(           \n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, 32, 2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 64, 2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 1, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    loss_train = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    trainiter = iter(trainloader)\n",
    "    \n",
    "    for idx_batch, data in enumerate(noiseloader):\n",
    "        \n",
    "        original = trainiter.next()\n",
    "        \n",
    "        if bCuda:\n",
    "            data = data.cuda()\n",
    "            original = original.cuda()\n",
    "        \n",
    "        data = Variable(data)\n",
    "        original = Variable(original)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = objective(output, original)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train_batch = loss.item()\n",
    "        loss_train.append(loss_train_batch)\n",
    "    \n",
    "    \n",
    "    loss_train_mean = np.mean(loss_train)\n",
    "    loss_train_std  = np.std(loss_train)\n",
    "    \n",
    "    return {'loss_train_mean': loss_train_mean, 'loss_train_std': loss_train_std }\n",
    "\n",
    "def validation():\n",
    "    \n",
    "    loss_val = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    valiter = iter(valloader)\n",
    "    \n",
    "    for idx_batch, data in enumerate(noise_valloader):\n",
    "        \n",
    "        original = valiter.next()\n",
    "        \n",
    "        if bCuda:\n",
    "            data = data.cuda()\n",
    "            original = original.cuda()\n",
    "            \n",
    "        output  = model(data)\n",
    "        loss    = objective(output, original)\n",
    "        \n",
    "        loss_val.append(loss.item())\n",
    "        \n",
    "    loss_val_mean = np.mean(loss_val)\n",
    "    \n",
    "    return {'loss_val_mean' : loss_val_mean, }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validate AutoEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder()\n",
    "\n",
    "bCuda = 1\n",
    "\n",
    "if bCuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer   = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-5)\n",
    "objective   = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 500\n",
    "loss_train_mean = []\n",
    "loss_train_std = []\n",
    "loss_val_mean = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch     1] loss: (training) 0.05675 \n",
      "[epoch     2] loss: (training) 0.01510 \n",
      "[epoch     3] loss: (training) 0.01021 \n",
      "[epoch     4] loss: (training) 0.00858 \n",
      "[epoch     5] loss: (training) 0.00762 \n",
      "[epoch     6] loss: (training) 0.00687 \n",
      "[epoch     7] loss: (training) 0.00633 \n",
      "[epoch     8] loss: (training) 0.00597 \n",
      "[epoch     9] loss: (training) 0.00570 \n",
      "[epoch    10] loss: (training) 0.00547 \n",
      "[epoch    11] loss: (training) 0.00528 \n",
      "[epoch    12] loss: (training) 0.00511 \n",
      "[epoch    13] loss: (training) 0.00497 \n",
      "[epoch    14] loss: (training) 0.00484 \n",
      "[epoch    15] loss: (training) 0.00473 \n",
      "[epoch    16] loss: (training) 0.00463 \n",
      "[epoch    17] loss: (training) 0.00453 \n",
      "[epoch    18] loss: (training) 0.00445 \n",
      "[epoch    19] loss: (training) 0.00437 \n",
      "[epoch    20] loss: (training) 0.00430 \n",
      "[epoch    21] loss: (training) 0.00423 \n",
      "[epoch    22] loss: (training) 0.00417 \n",
      "[epoch    23] loss: (training) 0.00411 \n",
      "[epoch    24] loss: (training) 0.00406 \n",
      "[epoch    25] loss: (training) 0.00401 \n",
      "[epoch    26] loss: (training) 0.00396 \n",
      "[epoch    27] loss: (training) 0.00392 \n",
      "[epoch    28] loss: (training) 0.00388 \n",
      "[epoch    29] loss: (training) 0.00384 \n",
      "[epoch    30] loss: (training) 0.00380 \n",
      "[epoch    31] loss: (training) 0.00377 \n",
      "[epoch    32] loss: (training) 0.00373 \n",
      "[epoch    33] loss: (training) 0.00370 \n",
      "[epoch    34] loss: (training) 0.00367 \n",
      "[epoch    35] loss: (training) 0.00364 \n",
      "[epoch    36] loss: (training) 0.00361 \n",
      "[epoch    37] loss: (training) 0.00359 \n",
      "[epoch    38] loss: (training) 0.00356 \n",
      "[epoch    39] loss: (training) 0.00353 \n",
      "[epoch    40] loss: (training) 0.00351 \n",
      "[epoch    41] loss: (training) 0.00348 \n",
      "[epoch    42] loss: (training) 0.00346 \n",
      "[epoch    43] loss: (training) 0.00344 \n",
      "[epoch    44] loss: (training) 0.00342 \n",
      "[epoch    45] loss: (training) 0.00340 \n",
      "[epoch    46] loss: (training) 0.00338 \n",
      "[epoch    47] loss: (training) 0.00336 \n",
      "[epoch    48] loss: (training) 0.00334 \n",
      "[epoch    49] loss: (training) 0.00332 \n",
      "[epoch    50] loss: (training) 0.00330 \n",
      "[epoch    51] loss: (training) 0.00329 \n",
      "[epoch    52] loss: (training) 0.00327 \n",
      "[epoch    53] loss: (training) 0.00325 \n",
      "[epoch    54] loss: (training) 0.00324 \n",
      "[epoch    55] loss: (training) 0.00322 \n",
      "[epoch    56] loss: (training) 0.00321 \n",
      "[epoch    57] loss: (training) 0.00319 \n",
      "[epoch    58] loss: (training) 0.00318 \n",
      "[epoch    59] loss: (training) 0.00317 \n",
      "[epoch    60] loss: (training) 0.00315 \n",
      "[epoch    61] loss: (training) 0.00314 \n",
      "[epoch    62] loss: (training) 0.00313 \n",
      "[epoch    63] loss: (training) 0.00311 \n",
      "[epoch    64] loss: (training) 0.00310 \n",
      "[epoch    65] loss: (training) 0.00309 \n",
      "[epoch    66] loss: (training) 0.00308 \n",
      "[epoch    67] loss: (training) 0.00307 \n",
      "[epoch    68] loss: (training) 0.00305 \n",
      "[epoch    69] loss: (training) 0.00304 \n",
      "[epoch    70] loss: (training) 0.00303 \n",
      "[epoch    71] loss: (training) 0.00302 \n",
      "[epoch    72] loss: (training) 0.00301 \n",
      "[epoch    73] loss: (training) 0.00300 \n",
      "[epoch    74] loss: (training) 0.00299 \n",
      "[epoch    75] loss: (training) 0.00298 \n",
      "[epoch    76] loss: (training) 0.00297 \n",
      "[epoch    77] loss: (training) 0.00296 \n",
      "[epoch    78] loss: (training) 0.00295 \n",
      "[epoch    79] loss: (training) 0.00294 \n",
      "[epoch    80] loss: (training) 0.00293 \n",
      "[epoch    81] loss: (training) 0.00293 \n",
      "[epoch    82] loss: (training) 0.00292 \n",
      "[epoch    83] loss: (training) 0.00291 \n",
      "[epoch    84] loss: (training) 0.00290 \n",
      "[epoch    85] loss: (training) 0.00289 \n",
      "[epoch    86] loss: (training) 0.00288 \n",
      "[epoch    87] loss: (training) 0.00288 \n",
      "[epoch    88] loss: (training) 0.00287 \n",
      "[epoch    89] loss: (training) 0.00286 \n",
      "[epoch    90] loss: (training) 0.00285 \n",
      "[epoch    91] loss: (training) 0.00285 \n",
      "[epoch    92] loss: (training) 0.00284 \n",
      "[epoch    93] loss: (training) 0.00283 \n",
      "[epoch    94] loss: (training) 0.00283 \n",
      "[epoch    95] loss: (training) 0.00282 \n",
      "[epoch    96] loss: (training) 0.00281 \n",
      "[epoch    97] loss: (training) 0.00280 \n",
      "[epoch    98] loss: (training) 0.00280 \n",
      "[epoch    99] loss: (training) 0.00279 \n",
      "[epoch   100] loss: (training) 0.00279 \n",
      "[epoch   101] loss: (training) 0.00278 \n",
      "[epoch   102] loss: (training) 0.00277 \n",
      "[epoch   103] loss: (training) 0.00277 \n",
      "[epoch   104] loss: (training) 0.00276 \n",
      "[epoch   105] loss: (training) 0.00276 \n",
      "[epoch   106] loss: (training) 0.00275 \n",
      "[epoch   107] loss: (training) 0.00274 \n",
      "[epoch   108] loss: (training) 0.00274 \n",
      "[epoch   109] loss: (training) 0.00273 \n",
      "[epoch   110] loss: (training) 0.00273 \n",
      "[epoch   111] loss: (training) 0.00272 \n",
      "[epoch   112] loss: (training) 0.00272 \n",
      "[epoch   113] loss: (training) 0.00271 \n",
      "[epoch   114] loss: (training) 0.00271 \n",
      "[epoch   115] loss: (training) 0.00270 \n",
      "[epoch   116] loss: (training) 0.00270 \n",
      "[epoch   117] loss: (training) 0.00269 \n",
      "[epoch   118] loss: (training) 0.00269 \n",
      "[epoch   119] loss: (training) 0.00268 \n",
      "[epoch   120] loss: (training) 0.00268 \n",
      "[epoch   121] loss: (training) 0.00267 \n",
      "[epoch   122] loss: (training) 0.00267 \n",
      "[epoch   123] loss: (training) 0.00267 \n",
      "[epoch   124] loss: (training) 0.00266 \n",
      "[epoch   125] loss: (training) 0.00266 \n",
      "[epoch   126] loss: (training) 0.00265 \n",
      "[epoch   127] loss: (training) 0.00265 \n",
      "[epoch   128] loss: (training) 0.00264 \n",
      "[epoch   129] loss: (training) 0.00264 \n",
      "[epoch   130] loss: (training) 0.00264 \n",
      "[epoch   131] loss: (training) 0.00263 \n",
      "[epoch   132] loss: (training) 0.00263 \n",
      "[epoch   133] loss: (training) 0.00262 \n",
      "[epoch   134] loss: (training) 0.00262 \n",
      "[epoch   135] loss: (training) 0.00262 \n",
      "[epoch   136] loss: (training) 0.00261 \n",
      "[epoch   137] loss: (training) 0.00261 \n",
      "[epoch   138] loss: (training) 0.00260 \n",
      "[epoch   139] loss: (training) 0.00260 \n",
      "[epoch   140] loss: (training) 0.00260 \n",
      "[epoch   141] loss: (training) 0.00259 \n",
      "[epoch   142] loss: (training) 0.00259 \n",
      "[epoch   143] loss: (training) 0.00258 \n",
      "[epoch   144] loss: (training) 0.00258 \n",
      "[epoch   145] loss: (training) 0.00258 \n",
      "[epoch   146] loss: (training) 0.00257 \n",
      "[epoch   147] loss: (training) 0.00257 \n",
      "[epoch   148] loss: (training) 0.00257 \n",
      "[epoch   149] loss: (training) 0.00256 \n",
      "[epoch   150] loss: (training) 0.00256 \n",
      "[epoch   151] loss: (training) 0.00255 \n",
      "[epoch   152] loss: (training) 0.00255 \n",
      "[epoch   153] loss: (training) 0.00255 \n",
      "[epoch   154] loss: (training) 0.00254 \n",
      "[epoch   155] loss: (training) 0.00254 \n",
      "[epoch   156] loss: (training) 0.00254 \n",
      "[epoch   157] loss: (training) 0.00253 \n",
      "[epoch   158] loss: (training) 0.00253 \n",
      "[epoch   159] loss: (training) 0.00253 \n",
      "[epoch   160] loss: (training) 0.00252 \n",
      "[epoch   161] loss: (training) 0.00252 \n",
      "[epoch   162] loss: (training) 0.00252 \n",
      "[epoch   163] loss: (training) 0.00251 \n",
      "[epoch   164] loss: (training) 0.00251 \n",
      "[epoch   165] loss: (training) 0.00251 \n",
      "[epoch   166] loss: (training) 0.00250 \n",
      "[epoch   167] loss: (training) 0.00250 \n",
      "[epoch   168] loss: (training) 0.00250 \n",
      "[epoch   169] loss: (training) 0.00250 \n",
      "[epoch   170] loss: (training) 0.00249 \n",
      "[epoch   171] loss: (training) 0.00249 \n",
      "[epoch   172] loss: (training) 0.00249 \n",
      "[epoch   173] loss: (training) 0.00248 \n",
      "[epoch   174] loss: (training) 0.00248 \n",
      "[epoch   175] loss: (training) 0.00248 \n",
      "[epoch   176] loss: (training) 0.00248 \n",
      "[epoch   177] loss: (training) 0.00247 \n",
      "[epoch   178] loss: (training) 0.00247 \n",
      "[epoch   179] loss: (training) 0.00247 \n",
      "[epoch   180] loss: (training) 0.00247 \n",
      "[epoch   181] loss: (training) 0.00246 \n",
      "[epoch   182] loss: (training) 0.00246 \n",
      "[epoch   183] loss: (training) 0.00246 \n",
      "[epoch   184] loss: (training) 0.00246 \n",
      "[epoch   185] loss: (training) 0.00245 \n",
      "[epoch   186] loss: (training) 0.00245 \n",
      "[epoch   187] loss: (training) 0.00245 \n",
      "[epoch   188] loss: (training) 0.00245 \n",
      "[epoch   189] loss: (training) 0.00244 \n",
      "[epoch   190] loss: (training) 0.00244 \n",
      "[epoch   191] loss: (training) 0.00244 \n",
      "[epoch   192] loss: (training) 0.00244 \n",
      "[epoch   193] loss: (training) 0.00243 \n",
      "[epoch   194] loss: (training) 0.00243 \n",
      "[epoch   195] loss: (training) 0.00243 \n",
      "[epoch   196] loss: (training) 0.00243 \n",
      "[epoch   197] loss: (training) 0.00242 \n",
      "[epoch   198] loss: (training) 0.00242 \n",
      "[epoch   199] loss: (training) 0.00242 \n",
      "[epoch   200] loss: (training) 0.00242 \n",
      "[epoch   201] loss: (training) 0.00241 \n",
      "[epoch   202] loss: (training) 0.00241 \n",
      "[epoch   203] loss: (training) 0.00241 \n",
      "[epoch   204] loss: (training) 0.00241 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch   205] loss: (training) 0.00241 \n",
      "[epoch   206] loss: (training) 0.00240 \n",
      "[epoch   207] loss: (training) 0.00240 \n",
      "[epoch   208] loss: (training) 0.00240 \n",
      "[epoch   209] loss: (training) 0.00240 \n",
      "[epoch   210] loss: (training) 0.00240 \n",
      "[epoch   211] loss: (training) 0.00239 \n",
      "[epoch   212] loss: (training) 0.00239 \n",
      "[epoch   213] loss: (training) 0.00239 \n",
      "[epoch   214] loss: (training) 0.00239 \n",
      "[epoch   215] loss: (training) 0.00238 \n",
      "[epoch   216] loss: (training) 0.00238 \n",
      "[epoch   217] loss: (training) 0.00238 \n",
      "[epoch   218] loss: (training) 0.00238 \n",
      "[epoch   219] loss: (training) 0.00238 \n",
      "[epoch   220] loss: (training) 0.00237 \n",
      "[epoch   221] loss: (training) 0.00237 \n",
      "[epoch   222] loss: (training) 0.00237 \n",
      "[epoch   223] loss: (training) 0.00237 \n",
      "[epoch   224] loss: (training) 0.00237 \n",
      "[epoch   225] loss: (training) 0.00236 \n",
      "[epoch   226] loss: (training) 0.00236 \n",
      "[epoch   227] loss: (training) 0.00236 \n",
      "[epoch   228] loss: (training) 0.00236 \n",
      "[epoch   229] loss: (training) 0.00236 \n",
      "[epoch   230] loss: (training) 0.00235 \n",
      "[epoch   231] loss: (training) 0.00235 \n",
      "[epoch   232] loss: (training) 0.00235 \n",
      "[epoch   233] loss: (training) 0.00235 \n",
      "[epoch   234] loss: (training) 0.00235 \n",
      "[epoch   235] loss: (training) 0.00234 \n",
      "[epoch   236] loss: (training) 0.00234 \n",
      "[epoch   237] loss: (training) 0.00234 \n",
      "[epoch   238] loss: (training) 0.00234 \n",
      "[epoch   239] loss: (training) 0.00234 \n",
      "[epoch   240] loss: (training) 0.00233 \n",
      "[epoch   241] loss: (training) 0.00233 \n",
      "[epoch   242] loss: (training) 0.00233 \n",
      "[epoch   243] loss: (training) 0.00233 \n",
      "[epoch   244] loss: (training) 0.00233 \n",
      "[epoch   245] loss: (training) 0.00232 \n",
      "[epoch   246] loss: (training) 0.00232 \n",
      "[epoch   247] loss: (training) 0.00232 \n",
      "[epoch   248] loss: (training) 0.00232 \n",
      "[epoch   249] loss: (training) 0.00232 \n",
      "[epoch   250] loss: (training) 0.00232 \n",
      "[epoch   251] loss: (training) 0.00231 \n",
      "[epoch   252] loss: (training) 0.00231 \n",
      "[epoch   253] loss: (training) 0.00231 \n",
      "[epoch   254] loss: (training) 0.00231 \n",
      "[epoch   255] loss: (training) 0.00231 \n",
      "[epoch   256] loss: (training) 0.00230 \n",
      "[epoch   257] loss: (training) 0.00230 \n",
      "[epoch   258] loss: (training) 0.00230 \n",
      "[epoch   259] loss: (training) 0.00230 \n",
      "[epoch   260] loss: (training) 0.00230 \n",
      "[epoch   261] loss: (training) 0.00230 \n",
      "[epoch   262] loss: (training) 0.00229 \n",
      "[epoch   263] loss: (training) 0.00229 \n",
      "[epoch   264] loss: (training) 0.00229 \n",
      "[epoch   265] loss: (training) 0.00229 \n",
      "[epoch   266] loss: (training) 0.00229 \n",
      "[epoch   267] loss: (training) 0.00229 \n",
      "[epoch   268] loss: (training) 0.00228 \n",
      "[epoch   269] loss: (training) 0.00228 \n",
      "[epoch   270] loss: (training) 0.00228 \n",
      "[epoch   271] loss: (training) 0.00228 \n",
      "[epoch   272] loss: (training) 0.00228 \n",
      "[epoch   273] loss: (training) 0.00228 \n",
      "[epoch   274] loss: (training) 0.00227 \n",
      "[epoch   275] loss: (training) 0.00227 \n",
      "[epoch   276] loss: (training) 0.00227 \n",
      "[epoch   277] loss: (training) 0.00227 \n",
      "[epoch   278] loss: (training) 0.00227 \n",
      "[epoch   279] loss: (training) 0.00227 \n",
      "[epoch   280] loss: (training) 0.00227 \n",
      "[epoch   281] loss: (training) 0.00226 \n",
      "[epoch   282] loss: (training) 0.00226 \n",
      "[epoch   283] loss: (training) 0.00226 \n",
      "[epoch   284] loss: (training) 0.00226 \n",
      "[epoch   285] loss: (training) 0.00226 \n",
      "[epoch   286] loss: (training) 0.00226 \n",
      "[epoch   287] loss: (training) 0.00225 \n",
      "[epoch   288] loss: (training) 0.00225 \n",
      "[epoch   289] loss: (training) 0.00225 \n",
      "[epoch   290] loss: (training) 0.00225 \n",
      "[epoch   291] loss: (training) 0.00225 \n",
      "[epoch   292] loss: (training) 0.00225 \n",
      "[epoch   293] loss: (training) 0.00225 \n",
      "[epoch   294] loss: (training) 0.00224 \n",
      "[epoch   295] loss: (training) 0.00224 \n",
      "[epoch   296] loss: (training) 0.00224 \n",
      "[epoch   297] loss: (training) 0.00224 \n",
      "[epoch   298] loss: (training) 0.00224 \n",
      "[epoch   299] loss: (training) 0.00224 \n",
      "[epoch   300] loss: (training) 0.00224 \n",
      "[epoch   301] loss: (training) 0.00223 \n",
      "[epoch   302] loss: (training) 0.00223 \n",
      "[epoch   303] loss: (training) 0.00223 \n",
      "[epoch   304] loss: (training) 0.00223 \n",
      "[epoch   305] loss: (training) 0.00223 \n",
      "[epoch   306] loss: (training) 0.00223 \n",
      "[epoch   307] loss: (training) 0.00222 \n",
      "[epoch   308] loss: (training) 0.00222 \n",
      "[epoch   309] loss: (training) 0.00222 \n",
      "[epoch   310] loss: (training) 0.00222 \n",
      "[epoch   311] loss: (training) 0.00222 \n",
      "[epoch   312] loss: (training) 0.00222 \n",
      "[epoch   313] loss: (training) 0.00222 \n",
      "[epoch   314] loss: (training) 0.00222 \n",
      "[epoch   315] loss: (training) 0.00221 \n",
      "[epoch   316] loss: (training) 0.00221 \n",
      "[epoch   317] loss: (training) 0.00221 \n",
      "[epoch   318] loss: (training) 0.00221 \n",
      "[epoch   319] loss: (training) 0.00221 \n",
      "[epoch   320] loss: (training) 0.00221 \n",
      "[epoch   321] loss: (training) 0.00221 \n",
      "[epoch   322] loss: (training) 0.00220 \n",
      "[epoch   323] loss: (training) 0.00220 \n",
      "[epoch   324] loss: (training) 0.00220 \n",
      "[epoch   325] loss: (training) 0.00220 \n",
      "[epoch   326] loss: (training) 0.00220 \n",
      "[epoch   327] loss: (training) 0.00220 \n",
      "[epoch   328] loss: (training) 0.00220 \n",
      "[epoch   329] loss: (training) 0.00219 \n",
      "[epoch   330] loss: (training) 0.00219 \n",
      "[epoch   331] loss: (training) 0.00219 \n",
      "[epoch   332] loss: (training) 0.00219 \n",
      "[epoch   333] loss: (training) 0.00219 \n",
      "[epoch   334] loss: (training) 0.00219 \n",
      "[epoch   335] loss: (training) 0.00219 \n",
      "[epoch   336] loss: (training) 0.00219 \n",
      "[epoch   337] loss: (training) 0.00218 \n",
      "[epoch   338] loss: (training) 0.00218 \n",
      "[epoch   339] loss: (training) 0.00218 \n",
      "[epoch   340] loss: (training) 0.00218 \n",
      "[epoch   341] loss: (training) 0.00218 \n",
      "[epoch   342] loss: (training) 0.00218 \n",
      "[epoch   343] loss: (training) 0.00218 \n",
      "[epoch   344] loss: (training) 0.00217 \n",
      "[epoch   345] loss: (training) 0.00217 \n",
      "[epoch   346] loss: (training) 0.00217 \n",
      "[epoch   347] loss: (training) 0.00217 \n",
      "[epoch   348] loss: (training) 0.00217 \n",
      "[epoch   349] loss: (training) 0.00217 \n",
      "[epoch   350] loss: (training) 0.00217 \n",
      "[epoch   351] loss: (training) 0.00217 \n",
      "[epoch   352] loss: (training) 0.00216 \n",
      "[epoch   353] loss: (training) 0.00216 \n",
      "[epoch   354] loss: (training) 0.00216 \n",
      "[epoch   355] loss: (training) 0.00216 \n",
      "[epoch   356] loss: (training) 0.00216 \n",
      "[epoch   357] loss: (training) 0.00216 \n",
      "[epoch   358] loss: (training) 0.00216 \n",
      "[epoch   359] loss: (training) 0.00216 \n",
      "[epoch   360] loss: (training) 0.00215 \n",
      "[epoch   361] loss: (training) 0.00215 \n",
      "[epoch   362] loss: (training) 0.00215 \n",
      "[epoch   363] loss: (training) 0.00215 \n",
      "[epoch   364] loss: (training) 0.00215 \n",
      "[epoch   365] loss: (training) 0.00215 \n",
      "[epoch   366] loss: (training) 0.00215 \n",
      "[epoch   367] loss: (training) 0.00214 \n",
      "[epoch   368] loss: (training) 0.00214 \n",
      "[epoch   369] loss: (training) 0.00214 \n",
      "[epoch   370] loss: (training) 0.00214 \n",
      "[epoch   371] loss: (training) 0.00214 \n",
      "[epoch   372] loss: (training) 0.00214 \n",
      "[epoch   373] loss: (training) 0.00214 \n",
      "[epoch   374] loss: (training) 0.00214 \n",
      "[epoch   375] loss: (training) 0.00213 \n",
      "[epoch   376] loss: (training) 0.00213 \n",
      "[epoch   377] loss: (training) 0.00213 \n",
      "[epoch   378] loss: (training) 0.00213 \n",
      "[epoch   379] loss: (training) 0.00213 \n",
      "[epoch   380] loss: (training) 0.00213 \n",
      "[epoch   381] loss: (training) 0.00213 \n",
      "[epoch   382] loss: (training) 0.00213 \n",
      "[epoch   383] loss: (training) 0.00212 \n",
      "[epoch   384] loss: (training) 0.00212 \n",
      "[epoch   385] loss: (training) 0.00212 \n",
      "[epoch   386] loss: (training) 0.00212 \n",
      "[epoch   387] loss: (training) 0.00212 \n",
      "[epoch   388] loss: (training) 0.00212 \n",
      "[epoch   389] loss: (training) 0.00212 \n",
      "[epoch   390] loss: (training) 0.00212 \n",
      "[epoch   391] loss: (training) 0.00212 \n",
      "[epoch   392] loss: (training) 0.00211 \n",
      "[epoch   393] loss: (training) 0.00211 \n",
      "[epoch   394] loss: (training) 0.00211 \n",
      "[epoch   395] loss: (training) 0.00211 \n",
      "[epoch   396] loss: (training) 0.00211 \n",
      "[epoch   397] loss: (training) 0.00211 \n",
      "[epoch   398] loss: (training) 0.00211 \n",
      "[epoch   399] loss: (training) 0.00211 \n",
      "[epoch   400] loss: (training) 0.00210 \n",
      "[epoch   401] loss: (training) 0.00210 \n",
      "[epoch   402] loss: (training) 0.00210 \n",
      "[epoch   403] loss: (training) 0.00210 \n",
      "[epoch   404] loss: (training) 0.00210 \n",
      "[epoch   405] loss: (training) 0.00210 \n",
      "[epoch   406] loss: (training) 0.00210 \n",
      "[epoch   407] loss: (training) 0.00210 \n",
      "[epoch   408] loss: (training) 0.00210 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch   409] loss: (training) 0.00209 \n",
      "[epoch   410] loss: (training) 0.00209 \n",
      "[epoch   411] loss: (training) 0.00209 \n",
      "[epoch   412] loss: (training) 0.00209 \n",
      "[epoch   413] loss: (training) 0.00209 \n",
      "[epoch   414] loss: (training) 0.00209 \n",
      "[epoch   415] loss: (training) 0.00209 \n",
      "[epoch   416] loss: (training) 0.00209 \n",
      "[epoch   417] loss: (training) 0.00208 \n",
      "[epoch   418] loss: (training) 0.00208 \n",
      "[epoch   419] loss: (training) 0.00208 \n",
      "[epoch   420] loss: (training) 0.00208 \n",
      "[epoch   421] loss: (training) 0.00208 \n",
      "[epoch   422] loss: (training) 0.00208 \n",
      "[epoch   423] loss: (training) 0.00208 \n",
      "[epoch   424] loss: (training) 0.00208 \n",
      "[epoch   425] loss: (training) 0.00208 \n",
      "[epoch   426] loss: (training) 0.00207 \n",
      "[epoch   427] loss: (training) 0.00207 \n",
      "[epoch   428] loss: (training) 0.00207 \n",
      "[epoch   429] loss: (training) 0.00207 \n",
      "[epoch   430] loss: (training) 0.00207 \n",
      "[epoch   431] loss: (training) 0.00207 \n",
      "[epoch   432] loss: (training) 0.00207 \n",
      "[epoch   433] loss: (training) 0.00207 \n",
      "[epoch   434] loss: (training) 0.00206 \n",
      "[epoch   435] loss: (training) 0.00206 \n",
      "[epoch   436] loss: (training) 0.00206 \n",
      "[epoch   437] loss: (training) 0.00206 \n",
      "[epoch   438] loss: (training) 0.00206 \n",
      "[epoch   439] loss: (training) 0.00206 \n",
      "[epoch   440] loss: (training) 0.00206 \n",
      "[epoch   441] loss: (training) 0.00206 \n",
      "[epoch   442] loss: (training) 0.00206 \n",
      "[epoch   443] loss: (training) 0.00205 \n",
      "[epoch   444] loss: (training) 0.00205 \n",
      "[epoch   445] loss: (training) 0.00205 \n",
      "[epoch   446] loss: (training) 0.00205 \n",
      "[epoch   447] loss: (training) 0.00205 \n",
      "[epoch   448] loss: (training) 0.00205 \n",
      "[epoch   449] loss: (training) 0.00205 \n",
      "[epoch   450] loss: (training) 0.00205 \n",
      "[epoch   451] loss: (training) 0.00205 \n",
      "[epoch   452] loss: (training) 0.00204 \n",
      "[epoch   453] loss: (training) 0.00204 \n",
      "[epoch   454] loss: (training) 0.00204 \n",
      "[epoch   455] loss: (training) 0.00204 \n",
      "[epoch   456] loss: (training) 0.00204 \n",
      "[epoch   457] loss: (training) 0.00204 \n",
      "[epoch   458] loss: (training) 0.00204 \n",
      "[epoch   459] loss: (training) 0.00204 \n",
      "[epoch   460] loss: (training) 0.00203 \n",
      "[epoch   461] loss: (training) 0.00203 \n",
      "[epoch   462] loss: (training) 0.00203 \n",
      "[epoch   463] loss: (training) 0.00203 \n",
      "[epoch   464] loss: (training) 0.00203 \n",
      "[epoch   465] loss: (training) 0.00203 \n",
      "[epoch   466] loss: (training) 0.00203 \n",
      "[epoch   467] loss: (training) 0.00203 \n",
      "[epoch   468] loss: (training) 0.00203 \n",
      "[epoch   469] loss: (training) 0.00202 \n",
      "[epoch   470] loss: (training) 0.00202 \n",
      "[epoch   471] loss: (training) 0.00202 \n",
      "[epoch   472] loss: (training) 0.00202 \n",
      "[epoch   473] loss: (training) 0.00202 \n",
      "[epoch   474] loss: (training) 0.00202 \n",
      "[epoch   475] loss: (training) 0.00202 \n",
      "[epoch   476] loss: (training) 0.00202 \n",
      "[epoch   477] loss: (training) 0.00202 \n",
      "[epoch   478] loss: (training) 0.00201 \n",
      "[epoch   479] loss: (training) 0.00201 \n",
      "[epoch   480] loss: (training) 0.00201 \n",
      "[epoch   481] loss: (training) 0.00201 \n",
      "[epoch   482] loss: (training) 0.00201 \n",
      "[epoch   483] loss: (training) 0.00201 \n",
      "[epoch   484] loss: (training) 0.00201 \n",
      "[epoch   485] loss: (training) 0.00201 \n",
      "[epoch   486] loss: (training) 0.00201 \n",
      "[epoch   487] loss: (training) 0.00200 \n",
      "[epoch   488] loss: (training) 0.00200 \n",
      "[epoch   489] loss: (training) 0.00200 \n",
      "[epoch   490] loss: (training) 0.00200 \n",
      "[epoch   491] loss: (training) 0.00200 \n",
      "[epoch   492] loss: (training) 0.00200 \n",
      "[epoch   493] loss: (training) 0.00200 \n",
      "[epoch   494] loss: (training) 0.00200 \n",
      "[epoch   495] loss: (training) 0.00199 \n",
      "[epoch   496] loss: (training) 0.00199 \n",
      "[epoch   497] loss: (training) 0.00199 \n",
      "[epoch   498] loss: (training) 0.00199 \n",
      "[epoch   499] loss: (training) 0.00199 \n",
      "[epoch   500] loss: (training) 0.00199 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "        \n",
    "    result_train = train()\n",
    "    \n",
    "    loss_train_mean.append(result_train['loss_train_mean'])\n",
    "    loss_train_std.append(result_train['loss_train_std'])\n",
    "    \n",
    "    result_val = validation()\n",
    "    \n",
    "    loss_val_mean.append(result_val['loss_val_mean'])\n",
    "    \n",
    "    print(\"[epoch %5d] loss: (training) %0.5f \" % (epoch+1, result_train['loss_train_mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the average and standard deviation of the training loss at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x18882fa0630>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_std = np.array(loss_train_std)\n",
    "train_loss = np.array(loss_train_mean)\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.plot(range(1, len(train_loss)+1), train_loss,'-r', label='train_loss')\n",
    "# plt.plot(range(1, len(loss_val_mean)+1), loss_val_mean,'-b', label='val_loss')\n",
    "plt.fill_between(range(1, len(train_loss)+1), train_loss+loss_std, train_loss-loss_std, facecolor='#eba4a4')\n",
    "plt.title(\"Loss Value\")\n",
    "plt.xlabel(\"Epoch\", fontsize=\"12\")\n",
    "plt.ylabel(\"Loss\", fontsize=\"12\")\n",
    "\n",
    "plt.legend(fontsize=\"12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data file of the denoising results for the testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing\n",
    "testdata        = np.load('data/test.npy')\n",
    "testdataset     = numpyDataset(testdata, transform)\n",
    "testloader      = DataLoader(testdataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "result_for_submit = None    # this is for submit file\n",
    "\n",
    "\n",
    "for batch_idx, data in enumerate(testloader):\n",
    "\n",
    "        result_of_test = data\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            result_for_submit = result_of_test\n",
    "        else:\n",
    "            try:\n",
    "                result_for_submit = torch.cat([result_for_submit, result_of_test], dim=0)\n",
    "\n",
    "            except RuntimeError:\n",
    "                transposed = torch.transpose(result_of_test, 2, 3)\n",
    "                result_for_submit = torch.cat([result_for_submit, transposed], dim=0)\n",
    "        \n",
    "# the submit_file.shape must be (400,1,120,80) \n",
    "submit_file = result_for_submit.detach().numpy()\n",
    "np.save('GyeongHyeon_Kim.npy', submit_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "893.976px",
    "left": "1259.56px",
    "right": "20px",
    "top": "95.9965px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
