{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([#transforms.Resize((1,10000)),\n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = '..\\\\Assignment#2\\\\horse-or-human\\\\train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=1)  \n",
    "\n",
    "\n",
    "validation_data_path = '..\\\\Assignment#2\\\\horse-or-human\\\\validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=256, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1, init_weights=True):\n",
    "\n",
    "        super(Linear, self).__init__()\n",
    "\n",
    "        self.number_class   = num_classes\n",
    "\n",
    "        _size_image     = 100* 100\n",
    "        _num1           = 50\n",
    "        _num2           = 50\n",
    "        \n",
    "        self.fc1        = nn.Linear(_size_image, _num1, bias=True)\n",
    "        self.fc2        = nn.Linear(_num1, _num2, bias=True)\n",
    "        self.fc3        = nn.Linear(_num2, num_classes, bias=True)\n",
    "\n",
    "        self.fc_layer1  = nn.Sequential(self.fc1, nn.Tanh())\n",
    "        self.fc_layer2  = nn.Sequential(self.fc2, nn.Tanh())\n",
    "        self.fc_layer3  = nn.Sequential(self.fc3, nn.Sigmoid())\n",
    "        \n",
    "        self.classifier = nn.Sequential(self.fc_layer1, self.fc_layer2, self.fc_layer3)\n",
    "        \n",
    "        if(init_weights):\n",
    "            self._initialize_weight()        \n",
    "        \n",
    "    def _initialize_weight(self):\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance (m, nn.Linear):\n",
    "                n = m.in_features\n",
    "                m.weight.data.uniform_(- 1.0 / math.sqrt(n), 1.0 / math.sqrt(n))\n",
    "\n",
    "                if m.bias is not None:\n",
    "\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# import packages\n",
    "# -----------------------------------------------------------------------------\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime \n",
    "import csv\n",
    "import configparser\n",
    "import argparse\n",
    "import platform\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# load neural network model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "num_classes = 2\n",
    "model = Linear(num_classes=num_classes)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Set the flag for using cuda\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "bCuda = 1\n",
    "\n",
    "if bCuda:\n",
    "    model.cuda()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# optimization algorithm\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "optimizer   = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-5)\n",
    "objective   = nn.CrossEntropyLoss()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# function for training the model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train():\n",
    "\n",
    "    loss_train     = []\n",
    "    accuracy_train = []\n",
    "    pre_batch_size = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for idx_batch, (data, target) in enumerate(trainloader):\n",
    "        \n",
    "        if len(data) < pre_batch_size:\n",
    "            continue\n",
    "        pre_batch_size = len(data)\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        if bCuda:\n",
    "            data, target    = data.cuda(), target.cuda()\n",
    "\n",
    "        data, target    = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        pred    = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        accuracy_test   = 100. * float(correct) / trainloader.batch_size\n",
    "        accuracy_train.append(accuracy_test)\n",
    "        \n",
    "        loss = objective(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train_batch = loss.item()\n",
    "        loss_train.append(loss_train_batch)\n",
    "    \n",
    "    \n",
    "    loss_train_mean = np.mean(loss_train)\n",
    "    loss_train_std  = np.std(loss_train)\n",
    "    accuracy_train_mean = np.mean(accuracy_train)\n",
    "    accuracy_train_std = np.std(accuracy_train)\n",
    "    \n",
    "    return {'loss_train_mean': loss_train_mean, 'loss_train_std': loss_train_std, 'accuracy_train': accuracy_train_mean, 'accuracy_train_std': accuracy_train_std }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# function for testing the model\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def test():\n",
    "\n",
    "    # print('test the model at given epoch')\n",
    "\n",
    "    accuracy_test   = []\n",
    "    loss_test       = 0\n",
    "    correct         = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for idx_batch, (data, target) in enumerate(valloader):\n",
    "\n",
    "        if bCuda:       \n",
    "            data, target    = data.cuda(), target.cuda()\n",
    "        data, target    = Variable(data), Variable(target)\n",
    "\n",
    "        output  = model(data)\n",
    "        loss    = objective(output, target)\n",
    "\n",
    "        loss_test   += loss.item()\n",
    "        pred        = output.data.max(1)[1]\n",
    "        correct     += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    loss_test       = loss_test\n",
    "    accuracy_test   = 100. * float(correct) / len(valloader.dataset)\n",
    "\n",
    "    return {'loss_test': loss_test, 'accuracy_test': accuracy_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# initialize lists for iterations\n",
    "# -----------------------------------------------------------------------------\n",
    "epoch = 500\n",
    "\n",
    "loss_train_mean = []\n",
    "loss_train_std = []\n",
    "loss_test = []\n",
    "accuracy_train = []\n",
    "accuracy_train_std = []\n",
    "accuracy_test = []\n",
    "lr_list = [0.1, 0.01, 0.005, 0.002, 0.0015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3acbe64ccb4ab8bc7fc83a404e5454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### CHANGE LEARNING_RATE 0.100 TO 0.100 ###############\n",
      "[epoch     1] loss: (training) 0.68588, (testing) 0.68222, accuracy: (training) 54.58984, (testing) 50.00000\n",
      "[epoch     2] loss: (training) 0.67255, (testing) 0.67055, accuracy: (training) 58.20312, (testing) 50.00000\n",
      "[epoch     3] loss: (training) 0.66317, (testing) 0.61843, accuracy: (training) 62.40234, (testing) 81.64062\n",
      "[epoch     4] loss: (training) 0.65724, (testing) 0.60891, accuracy: (training) 62.89062, (testing) 73.82812\n",
      "[epoch     5] loss: (training) 0.64461, (testing) 0.69413, accuracy: (training) 62.59766, (testing) 51.17188\n",
      "[epoch     6] loss: (training) 0.64104, (testing) 0.70844, accuracy: (training) 65.13672, (testing) 50.00000\n",
      "[epoch     7] loss: (training) 0.64991, (testing) 0.61917, accuracy: (training) 60.44922, (testing) 62.89062\n",
      "[epoch     8] loss: (training) 0.62815, (testing) 0.61464, accuracy: (training) 65.91797, (testing) 60.54688\n",
      "[epoch     9] loss: (training) 0.62799, (testing) 0.64210, accuracy: (training) 66.30859, (testing) 60.54688\n",
      "[epoch    10] loss: (training) 0.62862, (testing) 0.56242, accuracy: (training) 65.62500, (testing) 76.17188\n",
      "[epoch    11] loss: (training) 0.61376, (testing) 0.50634, accuracy: (training) 67.96875, (testing) 86.32812\n",
      "[epoch    12] loss: (training) 0.61699, (testing) 0.48599, accuracy: (training) 67.38281, (testing) 86.71875\n",
      "[epoch    13] loss: (training) 0.60508, (testing) 0.57050, accuracy: (training) 68.26172, (testing) 72.65625\n",
      "[epoch    14] loss: (training) 0.58073, (testing) 0.72196, accuracy: (training) 72.85156, (testing) 53.12500\n",
      "[epoch    15] loss: (training) 0.61145, (testing) 0.59614, accuracy: (training) 68.16406, (testing) 66.40625\n",
      "[epoch    16] loss: (training) 0.59068, (testing) 0.56550, accuracy: (training) 70.70312, (testing) 72.65625\n",
      "[epoch    17] loss: (training) 0.59290, (testing) 0.51362, accuracy: (training) 69.53125, (testing) 80.85938\n",
      "[epoch    18] loss: (training) 0.56275, (testing) 0.59708, accuracy: (training) 75.68359, (testing) 69.53125\n",
      "[epoch    19] loss: (training) 0.62192, (testing) 0.52178, accuracy: (training) 65.91797, (testing) 80.07812\n",
      "[epoch    20] loss: (training) 0.57257, (testing) 0.47319, accuracy: (training) 73.73047, (testing) 85.93750\n",
      "[epoch    21] loss: (training) 0.55633, (testing) 0.61776, accuracy: (training) 74.80469, (testing) 66.40625\n",
      "[epoch    22] loss: (training) 0.53846, (testing) 0.63258, accuracy: (training) 77.73438, (testing) 62.10938\n",
      "[epoch    23] loss: (training) 0.56423, (testing) 0.49978, accuracy: (training) 73.53516, (testing) 81.64062\n",
      "[epoch    24] loss: (training) 0.56658, (testing) 0.57845, accuracy: (training) 73.24219, (testing) 71.09375\n",
      "[epoch    25] loss: (training) 0.58498, (testing) 0.50175, accuracy: (training) 70.31250, (testing) 81.25000\n",
      "[epoch    26] loss: (training) 0.51200, (testing) 0.51424, accuracy: (training) 81.25000, (testing) 79.29688\n",
      "[epoch    27] loss: (training) 0.50783, (testing) 0.54490, accuracy: (training) 81.54297, (testing) 75.78125\n",
      "[epoch    28] loss: (training) 0.57396, (testing) 0.43679, accuracy: (training) 72.85156, (testing) 88.28125\n",
      "[epoch    29] loss: (training) 0.52229, (testing) 0.59757, accuracy: (training) 78.71094, (testing) 68.75000\n",
      "[epoch    30] loss: (training) 0.55280, (testing) 0.44156, accuracy: (training) 75.78125, (testing) 87.50000\n",
      "[epoch    31] loss: (training) 0.56489, (testing) 0.50951, accuracy: (training) 72.85156, (testing) 79.29688\n",
      "[epoch    32] loss: (training) 0.51328, (testing) 0.51035, accuracy: (training) 80.07812, (testing) 79.29688\n",
      "[epoch    33] loss: (training) 0.52621, (testing) 0.43287, accuracy: (training) 78.80859, (testing) 86.71875\n",
      "[epoch    34] loss: (training) 0.51560, (testing) 0.48386, accuracy: (training) 79.39453, (testing) 82.42188\n",
      "[epoch    35] loss: (training) 0.51040, (testing) 0.54548, accuracy: (training) 80.95703, (testing) 75.00000\n",
      "[epoch    36] loss: (training) 0.52564, (testing) 0.51595, accuracy: (training) 78.02734, (testing) 79.68750\n",
      "[epoch    37] loss: (training) 0.48151, (testing) 0.53265, accuracy: (training) 83.30078, (testing) 77.73438\n",
      "[epoch    38] loss: (training) 0.53917, (testing) 0.51282, accuracy: (training) 76.26953, (testing) 79.68750\n",
      "[epoch    39] loss: (training) 0.54183, (testing) 0.44460, accuracy: (training) 76.66016, (testing) 86.71875\n",
      "[epoch    40] loss: (training) 0.51554, (testing) 0.51167, accuracy: (training) 79.19922, (testing) 79.68750\n",
      "[epoch    41] loss: (training) 0.51447, (testing) 0.42513, accuracy: (training) 79.39453, (testing) 88.28125\n",
      "[epoch    42] loss: (training) 0.52118, (testing) 0.44234, accuracy: (training) 78.61328, (testing) 86.71875\n",
      "[epoch    43] loss: (training) 0.52273, (testing) 0.43618, accuracy: (training) 77.53906, (testing) 88.28125\n",
      "[epoch    44] loss: (training) 0.46767, (testing) 0.50028, accuracy: (training) 85.25391, (testing) 81.25000\n",
      "[epoch    45] loss: (training) 0.50490, (testing) 0.42765, accuracy: (training) 80.76172, (testing) 88.28125\n",
      "[epoch    46] loss: (training) 0.50996, (testing) 0.42532, accuracy: (training) 79.10156, (testing) 88.67188\n",
      "[epoch    47] loss: (training) 0.51290, (testing) 0.55748, accuracy: (training) 78.90625, (testing) 74.60938\n",
      "[epoch    48] loss: (training) 0.51022, (testing) 0.45724, accuracy: (training) 79.68750, (testing) 83.98438\n",
      "[epoch    49] loss: (training) 0.50909, (testing) 0.59124, accuracy: (training) 80.46875, (testing) 70.70312\n",
      "[epoch    50] loss: (training) 0.50817, (testing) 0.63416, accuracy: (training) 79.00391, (testing) 63.67188\n",
      "[epoch    51] loss: (training) 0.48400, (testing) 0.47661, accuracy: (training) 82.91016, (testing) 81.25000\n",
      "[epoch    52] loss: (training) 0.46820, (testing) 0.55617, accuracy: (training) 84.47266, (testing) 75.00000\n",
      "[epoch    53] loss: (training) 0.47594, (testing) 0.41915, accuracy: (training) 83.78906, (testing) 89.84375\n",
      "[epoch    54] loss: (training) 0.56270, (testing) 0.53143, accuracy: (training) 73.73047, (testing) 76.56250\n",
      "[epoch    55] loss: (training) 0.45824, (testing) 0.41959, accuracy: (training) 85.83984, (testing) 89.84375\n",
      "[epoch    56] loss: (training) 0.47356, (testing) 0.52214, accuracy: (training) 83.98438, (testing) 77.34375\n",
      "[epoch    57] loss: (training) 0.51123, (testing) 0.50805, accuracy: (training) 78.22266, (testing) 80.07812\n",
      "[epoch    58] loss: (training) 0.47525, (testing) 0.49187, accuracy: (training) 83.88672, (testing) 81.25000\n",
      "[epoch    59] loss: (training) 0.47684, (testing) 0.43264, accuracy: (training) 83.30078, (testing) 88.28125\n",
      "[epoch    60] loss: (training) 0.47709, (testing) 0.49407, accuracy: (training) 83.88672, (testing) 81.25000\n",
      "[epoch    61] loss: (training) 0.48359, (testing) 0.45362, accuracy: (training) 83.20312, (testing) 84.37500\n",
      "[epoch    62] loss: (training) 0.45475, (testing) 0.44836, accuracy: (training) 86.23047, (testing) 86.32812\n",
      "[epoch    63] loss: (training) 0.46896, (testing) 0.45956, accuracy: (training) 83.78906, (testing) 83.59375\n",
      "[epoch    64] loss: (training) 0.49042, (testing) 0.42378, accuracy: (training) 81.64062, (testing) 90.23438\n",
      "[epoch    65] loss: (training) 0.48422, (testing) 0.44658, accuracy: (training) 81.93359, (testing) 86.32812\n",
      "[epoch    66] loss: (training) 0.46938, (testing) 0.43969, accuracy: (training) 84.47266, (testing) 86.71875\n",
      "[epoch    67] loss: (training) 0.46125, (testing) 0.41907, accuracy: (training) 84.86328, (testing) 89.45312\n",
      "[epoch    68] loss: (training) 0.46651, (testing) 0.43252, accuracy: (training) 84.17969, (testing) 88.28125\n",
      "[epoch    69] loss: (training) 0.50801, (testing) 0.42523, accuracy: (training) 78.71094, (testing) 89.45312\n",
      "[epoch    70] loss: (training) 0.49308, (testing) 0.52281, accuracy: (training) 82.12891, (testing) 77.73438\n",
      "[epoch    71] loss: (training) 0.45451, (testing) 0.42416, accuracy: (training) 85.93750, (testing) 89.84375\n",
      "[epoch    72] loss: (training) 0.46215, (testing) 0.43433, accuracy: (training) 84.96094, (testing) 88.67188\n",
      "[epoch    73] loss: (training) 0.47715, (testing) 0.55846, accuracy: (training) 82.91016, (testing) 75.39062\n",
      "[epoch    74] loss: (training) 0.45738, (testing) 0.51272, accuracy: (training) 85.54688, (testing) 79.29688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch    75] loss: (training) 0.45761, (testing) 0.50706, accuracy: (training) 84.86328, (testing) 79.29688\n",
      "[epoch    76] loss: (training) 0.45923, (testing) 0.46980, accuracy: (training) 84.76562, (testing) 81.64062\n",
      "[epoch    77] loss: (training) 0.44320, (testing) 0.42489, accuracy: (training) 87.01172, (testing) 89.06250\n",
      "[epoch    78] loss: (training) 0.46440, (testing) 0.48607, accuracy: (training) 84.17969, (testing) 82.81250\n",
      "[epoch    79] loss: (training) 0.44564, (testing) 0.53312, accuracy: (training) 86.62109, (testing) 76.95312\n",
      "[epoch    80] loss: (training) 0.45906, (testing) 0.52337, accuracy: (training) 85.05859, (testing) 77.73438\n",
      "[epoch    81] loss: (training) 0.44257, (testing) 0.44512, accuracy: (training) 86.71875, (testing) 86.32812\n",
      "[epoch    82] loss: (training) 0.42340, (testing) 0.57065, accuracy: (training) 89.25781, (testing) 74.21875\n",
      "[epoch    83] loss: (training) 0.44902, (testing) 0.42778, accuracy: (training) 86.13281, (testing) 89.06250\n",
      "[epoch    84] loss: (training) 0.44657, (testing) 0.48539, accuracy: (training) 86.52344, (testing) 81.25000\n",
      "[epoch    85] loss: (training) 0.42068, (testing) 0.45223, accuracy: (training) 89.25781, (testing) 84.76562\n",
      "[epoch    86] loss: (training) 0.48850, (testing) 0.50641, accuracy: (training) 81.34766, (testing) 79.68750\n",
      "[epoch    87] loss: (training) 0.43338, (testing) 0.42439, accuracy: (training) 88.37891, (testing) 88.67188\n",
      "[epoch    88] loss: (training) 0.40804, (testing) 0.42218, accuracy: (training) 91.21094, (testing) 89.06250\n",
      "[epoch    89] loss: (training) 0.50413, (testing) 0.52963, accuracy: (training) 79.88281, (testing) 76.95312\n",
      "[epoch    90] loss: (training) 0.44815, (testing) 0.46694, accuracy: (training) 87.50000, (testing) 81.64062\n",
      "[epoch    91] loss: (training) 0.40519, (testing) 0.43961, accuracy: (training) 91.60156, (testing) 87.89062\n",
      "[epoch    92] loss: (training) 0.42606, (testing) 0.42940, accuracy: (training) 89.25781, (testing) 89.06250\n",
      "[epoch    93] loss: (training) 0.40213, (testing) 0.41625, accuracy: (training) 91.89453, (testing) 89.84375\n",
      "[epoch    94] loss: (training) 0.41556, (testing) 0.62241, accuracy: (training) 90.03906, (testing) 66.40625\n",
      "[epoch    95] loss: (training) 0.43913, (testing) 0.53787, accuracy: (training) 87.20703, (testing) 76.56250\n",
      "[epoch    96] loss: (training) 0.42774, (testing) 0.53913, accuracy: (training) 88.76953, (testing) 76.56250\n",
      "[epoch    97] loss: (training) 0.46161, (testing) 0.46568, accuracy: (training) 84.27734, (testing) 82.42188\n",
      "[epoch    98] loss: (training) 0.49337, (testing) 0.46496, accuracy: (training) 81.34766, (testing) 83.20312\n",
      "[epoch    99] loss: (training) 0.45301, (testing) 0.46297, accuracy: (training) 86.03516, (testing) 83.98438\n",
      "[epoch   100] loss: (training) 0.41178, (testing) 0.46179, accuracy: (training) 90.91797, (testing) 84.37500\n",
      "############### CHANGE LEARNING_RATE 0.100 TO 0.100 ###############\n",
      "[epoch   101] loss: (training) 0.40939, (testing) 0.44646, accuracy: (training) 90.91797, (testing) 85.93750\n",
      "[epoch   102] loss: (training) 0.43901, (testing) 0.41439, accuracy: (training) 87.30469, (testing) 90.23438\n",
      "[epoch   103] loss: (training) 0.45241, (testing) 0.52320, accuracy: (training) 85.35156, (testing) 78.51562\n",
      "[epoch   104] loss: (training) 0.41796, (testing) 0.51027, accuracy: (training) 90.33203, (testing) 80.07812\n",
      "[epoch   105] loss: (training) 0.41315, (testing) 0.55855, accuracy: (training) 90.33203, (testing) 74.60938\n",
      "[epoch   106] loss: (training) 0.45571, (testing) 0.42227, accuracy: (training) 85.35156, (testing) 89.06250\n",
      "[epoch   107] loss: (training) 0.39787, (testing) 0.43239, accuracy: (training) 92.08984, (testing) 88.67188\n",
      "[epoch   108] loss: (training) 0.43048, (testing) 0.49621, accuracy: (training) 88.76953, (testing) 79.29688\n",
      "[epoch   109] loss: (training) 0.39021, (testing) 0.52593, accuracy: (training) 93.65234, (testing) 78.12500\n",
      "[epoch   110] loss: (training) 0.39676, (testing) 0.53505, accuracy: (training) 92.18750, (testing) 76.95312\n",
      "[epoch   111] loss: (training) 0.45010, (testing) 0.46626, accuracy: (training) 86.13281, (testing) 83.59375\n",
      "[epoch   112] loss: (training) 0.38699, (testing) 0.42860, accuracy: (training) 93.45703, (testing) 88.28125\n",
      "[epoch   113] loss: (training) 0.42204, (testing) 0.45366, accuracy: (training) 89.16016, (testing) 85.54688\n",
      "[epoch   114] loss: (training) 0.38365, (testing) 0.40488, accuracy: (training) 93.94531, (testing) 91.40625\n",
      "[epoch   115] loss: (training) 0.37789, (testing) 0.40532, accuracy: (training) 94.43359, (testing) 91.01562\n",
      "[epoch   116] loss: (training) 0.41557, (testing) 0.45517, accuracy: (training) 90.23438, (testing) 84.76562\n",
      "[epoch   117] loss: (training) 0.37495, (testing) 0.40553, accuracy: (training) 95.01953, (testing) 89.84375\n",
      "[epoch   118] loss: (training) 0.41605, (testing) 0.41964, accuracy: (training) 90.23438, (testing) 89.45312\n",
      "[epoch   119] loss: (training) 0.39003, (testing) 0.48374, accuracy: (training) 92.57812, (testing) 80.85938\n",
      "[epoch   120] loss: (training) 0.37258, (testing) 0.40392, accuracy: (training) 94.72656, (testing) 92.18750\n",
      "[epoch   121] loss: (training) 0.39942, (testing) 0.43660, accuracy: (training) 92.08984, (testing) 87.50000\n",
      "[epoch   122] loss: (training) 0.37502, (testing) 0.43521, accuracy: (training) 94.92188, (testing) 87.50000\n",
      "[epoch   123] loss: (training) 0.40249, (testing) 0.54683, accuracy: (training) 91.30859, (testing) 76.17188\n",
      "[epoch   124] loss: (training) 0.41397, (testing) 0.41013, accuracy: (training) 90.52734, (testing) 91.01562\n",
      "[epoch   125] loss: (training) 0.36417, (testing) 0.44628, accuracy: (training) 95.80078, (testing) 85.93750\n",
      "[epoch   126] loss: (training) 0.37506, (testing) 0.51843, accuracy: (training) 94.72656, (testing) 76.56250\n",
      "[epoch   127] loss: (training) 0.40184, (testing) 0.41373, accuracy: (training) 91.40625, (testing) 89.45312\n",
      "[epoch   128] loss: (training) 0.36783, (testing) 0.45422, accuracy: (training) 95.11719, (testing) 85.15625\n",
      "[epoch   129] loss: (training) 0.42010, (testing) 0.53254, accuracy: (training) 89.35547, (testing) 75.78125\n",
      "[epoch   130] loss: (training) 0.37648, (testing) 0.41670, accuracy: (training) 94.82422, (testing) 89.84375\n",
      "[epoch   131] loss: (training) 0.36498, (testing) 0.44388, accuracy: (training) 95.89844, (testing) 85.54688\n",
      "[epoch   132] loss: (training) 0.36005, (testing) 0.40691, accuracy: (training) 96.19141, (testing) 91.40625\n",
      "[epoch   133] loss: (training) 0.37670, (testing) 0.43770, accuracy: (training) 93.94531, (testing) 87.10938\n",
      "[epoch   134] loss: (training) 0.35683, (testing) 0.40849, accuracy: (training) 96.38672, (testing) 91.40625\n",
      "[epoch   135] loss: (training) 0.35585, (testing) 0.45123, accuracy: (training) 96.28906, (testing) 84.76562\n",
      "[epoch   136] loss: (training) 0.36069, (testing) 0.49670, accuracy: (training) 96.48438, (testing) 79.29688\n",
      "[epoch   137] loss: (training) 0.40497, (testing) 0.44154, accuracy: (training) 91.40625, (testing) 86.71875\n",
      "[epoch   138] loss: (training) 0.39032, (testing) 0.49607, accuracy: (training) 92.87109, (testing) 81.64062\n",
      "[epoch   139] loss: (training) 0.37928, (testing) 0.48742, accuracy: (training) 93.84766, (testing) 81.25000\n",
      "[epoch   140] loss: (training) 0.36575, (testing) 0.40784, accuracy: (training) 95.50781, (testing) 90.62500\n",
      "[epoch   141] loss: (training) 0.40884, (testing) 0.49756, accuracy: (training) 90.23438, (testing) 80.07812\n",
      "[epoch   142] loss: (training) 0.38789, (testing) 0.48793, accuracy: (training) 92.87109, (testing) 80.85938\n",
      "[epoch   143] loss: (training) 0.35546, (testing) 0.44765, accuracy: (training) 96.28906, (testing) 84.76562\n",
      "[epoch   144] loss: (training) 0.35000, (testing) 0.40678, accuracy: (training) 96.77734, (testing) 91.40625\n",
      "[epoch   145] loss: (training) 0.35761, (testing) 0.42690, accuracy: (training) 96.28906, (testing) 87.89062\n",
      "[epoch   146] loss: (training) 0.34765, (testing) 0.40796, accuracy: (training) 97.16797, (testing) 91.40625\n",
      "[epoch   147] loss: (training) 0.35252, (testing) 0.47479, accuracy: (training) 96.77734, (testing) 81.64062\n",
      "[epoch   148] loss: (training) 0.34664, (testing) 0.41526, accuracy: (training) 97.26562, (testing) 89.45312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch   149] loss: (training) 0.34897, (testing) 0.41332, accuracy: (training) 96.97266, (testing) 89.45312\n",
      "[epoch   150] loss: (training) 0.37549, (testing) 0.63574, accuracy: (training) 94.43359, (testing) 66.40625\n",
      "[epoch   151] loss: (training) 0.37867, (testing) 0.57808, accuracy: (training) 93.45703, (testing) 71.48438\n",
      "[epoch   152] loss: (training) 0.38045, (testing) 0.43024, accuracy: (training) 93.45703, (testing) 87.50000\n",
      "[epoch   153] loss: (training) 0.34804, (testing) 0.52315, accuracy: (training) 97.16797, (testing) 78.12500\n",
      "[epoch   154] loss: (training) 0.39761, (testing) 0.44461, accuracy: (training) 91.50391, (testing) 85.15625\n",
      "[epoch   155] loss: (training) 0.34674, (testing) 0.43102, accuracy: (training) 97.07031, (testing) 86.71875\n",
      "[epoch   156] loss: (training) 0.34219, (testing) 0.41186, accuracy: (training) 97.75391, (testing) 91.01562\n",
      "[epoch   157] loss: (training) 0.34128, (testing) 0.41082, accuracy: (training) 97.65625, (testing) 90.23438\n",
      "[epoch   158] loss: (training) 0.34381, (testing) 0.45764, accuracy: (training) 97.55859, (testing) 83.59375\n",
      "[epoch   159] loss: (training) 0.34567, (testing) 0.43681, accuracy: (training) 97.36328, (testing) 85.93750\n",
      "[epoch   160] loss: (training) 0.33860, (testing) 0.41464, accuracy: (training) 98.33984, (testing) 90.23438\n",
      "[epoch   161] loss: (training) 0.34027, (testing) 0.42960, accuracy: (training) 97.85156, (testing) 87.10938\n",
      "[epoch   162] loss: (training) 0.33799, (testing) 0.44214, accuracy: (training) 98.33984, (testing) 86.32812\n",
      "[epoch   163] loss: (training) 0.33746, (testing) 0.43204, accuracy: (training) 98.04688, (testing) 86.71875\n",
      "[epoch   164] loss: (training) 0.33601, (testing) 0.43230, accuracy: (training) 98.33984, (testing) 87.10938\n",
      "[epoch   165] loss: (training) 0.33595, (testing) 0.46341, accuracy: (training) 98.43750, (testing) 83.59375\n",
      "[epoch   166] loss: (training) 0.33676, (testing) 0.44697, accuracy: (training) 98.63281, (testing) 84.37500\n",
      "[epoch   167] loss: (training) 0.34562, (testing) 0.48779, accuracy: (training) 97.65625, (testing) 82.03125\n",
      "[epoch   168] loss: (training) 0.47999, (testing) 0.47609, accuracy: (training) 81.73828, (testing) 84.37500\n",
      "[epoch   169] loss: (training) 0.43562, (testing) 0.44718, accuracy: (training) 87.98828, (testing) 84.37500\n",
      "[epoch   170] loss: (training) 0.34131, (testing) 0.40976, accuracy: (training) 97.94922, (testing) 89.84375\n",
      "[epoch   171] loss: (training) 0.36947, (testing) 0.44556, accuracy: (training) 94.72656, (testing) 85.54688\n",
      "[epoch   172] loss: (training) 0.33591, (testing) 0.45827, accuracy: (training) 98.63281, (testing) 84.37500\n",
      "[epoch   173] loss: (training) 0.33700, (testing) 0.45739, accuracy: (training) 98.14453, (testing) 83.20312\n",
      "[epoch   174] loss: (training) 0.33677, (testing) 0.44128, accuracy: (training) 98.43750, (testing) 85.15625\n",
      "[epoch   175] loss: (training) 0.33433, (testing) 0.42103, accuracy: (training) 98.63281, (testing) 88.28125\n",
      "[epoch   176] loss: (training) 0.33294, (testing) 0.41532, accuracy: (training) 98.63281, (testing) 91.01562\n",
      "[epoch   177] loss: (training) 0.33217, (testing) 0.45772, accuracy: (training) 98.82812, (testing) 82.81250\n",
      "[epoch   178] loss: (training) 0.33167, (testing) 0.44105, accuracy: (training) 98.92578, (testing) 85.54688\n",
      "[epoch   179] loss: (training) 0.33212, (testing) 0.42486, accuracy: (training) 98.63281, (testing) 88.28125\n",
      "[epoch   180] loss: (training) 0.33128, (testing) 0.40827, accuracy: (training) 98.73047, (testing) 91.79688\n",
      "[epoch   181] loss: (training) 0.33021, (testing) 0.41266, accuracy: (training) 98.92578, (testing) 91.40625\n",
      "[epoch   182] loss: (training) 0.33078, (testing) 0.44861, accuracy: (training) 98.73047, (testing) 84.37500\n",
      "[epoch   183] loss: (training) 0.32996, (testing) 0.42343, accuracy: (training) 98.82812, (testing) 88.67188\n",
      "[epoch   184] loss: (training) 0.32958, (testing) 0.41359, accuracy: (training) 98.92578, (testing) 90.23438\n",
      "[epoch   185] loss: (training) 0.32934, (testing) 0.44996, accuracy: (training) 98.82812, (testing) 84.37500\n",
      "[epoch   186] loss: (training) 0.33087, (testing) 0.43497, accuracy: (training) 98.82812, (testing) 86.71875\n",
      "[epoch   187] loss: (training) 0.32869, (testing) 0.41935, accuracy: (training) 98.92578, (testing) 89.06250\n",
      "[epoch   188] loss: (training) 0.32890, (testing) 0.44229, accuracy: (training) 98.92578, (testing) 85.15625\n",
      "[epoch   189] loss: (training) 0.33012, (testing) 0.41891, accuracy: (training) 98.82812, (testing) 89.06250\n",
      "[epoch   190] loss: (training) 0.32818, (testing) 0.42417, accuracy: (training) 99.02344, (testing) 88.28125\n",
      "[epoch   191] loss: (training) 0.32846, (testing) 0.41311, accuracy: (training) 99.02344, (testing) 91.01562\n",
      "[epoch   192] loss: (training) 0.32775, (testing) 0.43669, accuracy: (training) 99.02344, (testing) 85.54688\n",
      "[epoch   193] loss: (training) 0.32737, (testing) 0.43061, accuracy: (training) 99.02344, (testing) 87.10938\n",
      "[epoch   194] loss: (training) 0.32805, (testing) 0.42322, accuracy: (training) 99.02344, (testing) 88.28125\n",
      "[epoch   195] loss: (training) 0.32738, (testing) 0.43316, accuracy: (training) 99.02344, (testing) 86.71875\n",
      "[epoch   196] loss: (training) 0.32710, (testing) 0.42439, accuracy: (training) 99.02344, (testing) 87.89062\n",
      "[epoch   197] loss: (training) 0.32691, (testing) 0.42299, accuracy: (training) 99.02344, (testing) 88.67188\n",
      "[epoch   198] loss: (training) 0.32705, (testing) 0.42165, accuracy: (training) 99.02344, (testing) 88.67188\n",
      "[epoch   199] loss: (training) 0.32672, (testing) 0.45197, accuracy: (training) 99.02344, (testing) 84.37500\n",
      "[epoch   200] loss: (training) 0.32697, (testing) 0.43056, accuracy: (training) 99.02344, (testing) 87.50000\n",
      "############### CHANGE LEARNING_RATE 0.100 TO 0.010 ###############\n",
      "[epoch   201] loss: (training) 0.32622, (testing) 0.42884, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   202] loss: (training) 0.32616, (testing) 0.42655, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   203] loss: (training) 0.32613, (testing) 0.42640, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   204] loss: (training) 0.32617, (testing) 0.42648, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   205] loss: (training) 0.32615, (testing) 0.42599, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   206] loss: (training) 0.32614, (testing) 0.42623, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   207] loss: (training) 0.32613, (testing) 0.42637, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   208] loss: (training) 0.32614, (testing) 0.42668, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   209] loss: (training) 0.32611, (testing) 0.42548, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   210] loss: (training) 0.32610, (testing) 0.42579, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   211] loss: (training) 0.32609, (testing) 0.42571, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   212] loss: (training) 0.32607, (testing) 0.42579, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   213] loss: (training) 0.32607, (testing) 0.42543, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   214] loss: (training) 0.32606, (testing) 0.42599, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   215] loss: (training) 0.32604, (testing) 0.42570, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   216] loss: (training) 0.32603, (testing) 0.42512, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   217] loss: (training) 0.32603, (testing) 0.42573, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   218] loss: (training) 0.32601, (testing) 0.42570, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   219] loss: (training) 0.32603, (testing) 0.42520, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   220] loss: (training) 0.32599, (testing) 0.42529, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   221] loss: (training) 0.32601, (testing) 0.42505, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   222] loss: (training) 0.32597, (testing) 0.42460, accuracy: (training) 99.12109, (testing) 88.28125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch   223] loss: (training) 0.32599, (testing) 0.42545, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   224] loss: (training) 0.32500, (testing) 0.42562, accuracy: (training) 99.21875, (testing) 87.89062\n",
      "[epoch   225] loss: (training) 0.32597, (testing) 0.42552, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   226] loss: (training) 0.32596, (testing) 0.42500, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   227] loss: (training) 0.32594, (testing) 0.42522, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   228] loss: (training) 0.32592, (testing) 0.42490, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   229] loss: (training) 0.32594, (testing) 0.42534, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   230] loss: (training) 0.32590, (testing) 0.42529, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   231] loss: (training) 0.32589, (testing) 0.42505, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   232] loss: (training) 0.32589, (testing) 0.42465, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   233] loss: (training) 0.32590, (testing) 0.42444, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   234] loss: (training) 0.32588, (testing) 0.42401, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   235] loss: (training) 0.32587, (testing) 0.42434, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   236] loss: (training) 0.32586, (testing) 0.42440, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   237] loss: (training) 0.32585, (testing) 0.42408, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   238] loss: (training) 0.32583, (testing) 0.42510, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   239] loss: (training) 0.32547, (testing) 0.42398, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   240] loss: (training) 0.32583, (testing) 0.42443, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   241] loss: (training) 0.32582, (testing) 0.42503, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   242] loss: (training) 0.32582, (testing) 0.42472, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   243] loss: (training) 0.32579, (testing) 0.42448, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   244] loss: (training) 0.32579, (testing) 0.42525, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   245] loss: (training) 0.32579, (testing) 0.42482, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   246] loss: (training) 0.32481, (testing) 0.42450, accuracy: (training) 99.21875, (testing) 88.28125\n",
      "[epoch   247] loss: (training) 0.32575, (testing) 0.42456, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   248] loss: (training) 0.32577, (testing) 0.42441, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   249] loss: (training) 0.32576, (testing) 0.42512, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   250] loss: (training) 0.32570, (testing) 0.42425, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   251] loss: (training) 0.32574, (testing) 0.42407, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   252] loss: (training) 0.32574, (testing) 0.42387, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   253] loss: (training) 0.32572, (testing) 0.42377, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   254] loss: (training) 0.32571, (testing) 0.42455, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   255] loss: (training) 0.32568, (testing) 0.42511, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   256] loss: (training) 0.32569, (testing) 0.42453, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   257] loss: (training) 0.32566, (testing) 0.42387, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   258] loss: (training) 0.32569, (testing) 0.42403, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   259] loss: (training) 0.32566, (testing) 0.42465, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   260] loss: (training) 0.32562, (testing) 0.42440, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   261] loss: (training) 0.32566, (testing) 0.42400, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   262] loss: (training) 0.32565, (testing) 0.42403, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   263] loss: (training) 0.32563, (testing) 0.42416, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   264] loss: (training) 0.32562, (testing) 0.42428, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   265] loss: (training) 0.32563, (testing) 0.42447, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   266] loss: (training) 0.32560, (testing) 0.42471, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   267] loss: (training) 0.32561, (testing) 0.42391, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   268] loss: (training) 0.32560, (testing) 0.42407, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   269] loss: (training) 0.32558, (testing) 0.42427, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   270] loss: (training) 0.32556, (testing) 0.42423, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   271] loss: (training) 0.32557, (testing) 0.42478, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   272] loss: (training) 0.32557, (testing) 0.42477, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   273] loss: (training) 0.32556, (testing) 0.42458, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   274] loss: (training) 0.32555, (testing) 0.42453, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   275] loss: (training) 0.32555, (testing) 0.42458, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   276] loss: (training) 0.32551, (testing) 0.42407, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   277] loss: (training) 0.32553, (testing) 0.42407, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   278] loss: (training) 0.32552, (testing) 0.42483, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   279] loss: (training) 0.32552, (testing) 0.42419, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   280] loss: (training) 0.32550, (testing) 0.42448, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   281] loss: (training) 0.32549, (testing) 0.42453, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   282] loss: (training) 0.32550, (testing) 0.42425, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   283] loss: (training) 0.32531, (testing) 0.42416, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   284] loss: (training) 0.32548, (testing) 0.42436, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   285] loss: (training) 0.32452, (testing) 0.42481, accuracy: (training) 99.21875, (testing) 88.28125\n",
      "[epoch   286] loss: (training) 0.32547, (testing) 0.42460, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   287] loss: (training) 0.32546, (testing) 0.42435, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   288] loss: (training) 0.32545, (testing) 0.42443, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   289] loss: (training) 0.32544, (testing) 0.42440, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   290] loss: (training) 0.32543, (testing) 0.42458, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   291] loss: (training) 0.32542, (testing) 0.42465, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   292] loss: (training) 0.32541, (testing) 0.42439, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   293] loss: (training) 0.32541, (testing) 0.42411, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   294] loss: (training) 0.32541, (testing) 0.42421, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   295] loss: (training) 0.32540, (testing) 0.42421, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   296] loss: (training) 0.32539, (testing) 0.42410, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   297] loss: (training) 0.32538, (testing) 0.42411, accuracy: (training) 99.12109, (testing) 88.28125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch   298] loss: (training) 0.32535, (testing) 0.42430, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   299] loss: (training) 0.32534, (testing) 0.42475, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   300] loss: (training) 0.32537, (testing) 0.42433, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "############### CHANGE LEARNING_RATE 0.010 TO 0.010 ###############\n",
      "[epoch   301] loss: (training) 0.32536, (testing) 0.42404, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   302] loss: (training) 0.32535, (testing) 0.42375, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   303] loss: (training) 0.32535, (testing) 0.42405, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   304] loss: (training) 0.32533, (testing) 0.42367, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   305] loss: (training) 0.32532, (testing) 0.42394, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   306] loss: (training) 0.32532, (testing) 0.42393, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   307] loss: (training) 0.32532, (testing) 0.42416, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   308] loss: (training) 0.32531, (testing) 0.42355, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   309] loss: (training) 0.32530, (testing) 0.42382, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   310] loss: (training) 0.32529, (testing) 0.42392, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   311] loss: (training) 0.32527, (testing) 0.42371, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   312] loss: (training) 0.32526, (testing) 0.42405, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   313] loss: (training) 0.32525, (testing) 0.42430, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   314] loss: (training) 0.32528, (testing) 0.42415, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   315] loss: (training) 0.32527, (testing) 0.42414, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   316] loss: (training) 0.32526, (testing) 0.42399, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   317] loss: (training) 0.32525, (testing) 0.42369, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   318] loss: (training) 0.32523, (testing) 0.42366, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   319] loss: (training) 0.32427, (testing) 0.42359, accuracy: (training) 99.21875, (testing) 87.89062\n",
      "[epoch   320] loss: (training) 0.32522, (testing) 0.42371, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   321] loss: (training) 0.32522, (testing) 0.42415, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   322] loss: (training) 0.32522, (testing) 0.42436, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   323] loss: (training) 0.32426, (testing) 0.42401, accuracy: (training) 99.21875, (testing) 88.28125\n",
      "[epoch   324] loss: (training) 0.32520, (testing) 0.42373, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   325] loss: (training) 0.32521, (testing) 0.42324, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   326] loss: (training) 0.32521, (testing) 0.42284, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   327] loss: (training) 0.32519, (testing) 0.42364, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   328] loss: (training) 0.32516, (testing) 0.42364, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   329] loss: (training) 0.32518, (testing) 0.42362, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   330] loss: (training) 0.32516, (testing) 0.42362, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   331] loss: (training) 0.32511, (testing) 0.42360, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   332] loss: (training) 0.32517, (testing) 0.42340, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   333] loss: (training) 0.32515, (testing) 0.42353, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   334] loss: (training) 0.32516, (testing) 0.42345, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   335] loss: (training) 0.32512, (testing) 0.42340, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   336] loss: (training) 0.32513, (testing) 0.42330, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   337] loss: (training) 0.32512, (testing) 0.42322, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   338] loss: (training) 0.32511, (testing) 0.42360, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   339] loss: (training) 0.32511, (testing) 0.42350, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   340] loss: (training) 0.32510, (testing) 0.42398, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   341] loss: (training) 0.32510, (testing) 0.42334, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   342] loss: (training) 0.32508, (testing) 0.42340, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   343] loss: (training) 0.32508, (testing) 0.42360, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   344] loss: (training) 0.32507, (testing) 0.42338, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   345] loss: (training) 0.32506, (testing) 0.42331, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   346] loss: (training) 0.32504, (testing) 0.42306, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   347] loss: (training) 0.32506, (testing) 0.42339, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   348] loss: (training) 0.32506, (testing) 0.42281, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   349] loss: (training) 0.32505, (testing) 0.42272, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   350] loss: (training) 0.32504, (testing) 0.42304, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   351] loss: (training) 0.32504, (testing) 0.42304, accuracy: (training) 99.12109, (testing) 87.89062\n",
      "[epoch   352] loss: (training) 0.32504, (testing) 0.42297, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   353] loss: (training) 0.32503, (testing) 0.42279, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   354] loss: (training) 0.32503, (testing) 0.42262, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   355] loss: (training) 0.32502, (testing) 0.42253, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   356] loss: (training) 0.32501, (testing) 0.42266, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   357] loss: (training) 0.32500, (testing) 0.42259, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   358] loss: (training) 0.32501, (testing) 0.42286, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   359] loss: (training) 0.32498, (testing) 0.42293, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   360] loss: (training) 0.32499, (testing) 0.42244, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   361] loss: (training) 0.32498, (testing) 0.42222, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   362] loss: (training) 0.32497, (testing) 0.42249, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   363] loss: (training) 0.32495, (testing) 0.42218, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   364] loss: (training) 0.32495, (testing) 0.42200, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   365] loss: (training) 0.32495, (testing) 0.42229, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   366] loss: (training) 0.32495, (testing) 0.42227, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   367] loss: (training) 0.32494, (testing) 0.42180, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   368] loss: (training) 0.32495, (testing) 0.42198, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   369] loss: (training) 0.32494, (testing) 0.42156, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   370] loss: (training) 0.32491, (testing) 0.42140, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   371] loss: (training) 0.32493, (testing) 0.42197, accuracy: (training) 99.12109, (testing) 88.28125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch   372] loss: (training) 0.32491, (testing) 0.42196, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   373] loss: (training) 0.32490, (testing) 0.42195, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   374] loss: (training) 0.32489, (testing) 0.42162, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   375] loss: (training) 0.32488, (testing) 0.42136, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   376] loss: (training) 0.32487, (testing) 0.42195, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   377] loss: (training) 0.32488, (testing) 0.42141, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   378] loss: (training) 0.32488, (testing) 0.42137, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   379] loss: (training) 0.32487, (testing) 0.42124, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   380] loss: (training) 0.32485, (testing) 0.42126, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   381] loss: (training) 0.32482, (testing) 0.42184, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   382] loss: (training) 0.32487, (testing) 0.42131, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   383] loss: (training) 0.32482, (testing) 0.42067, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   384] loss: (training) 0.32484, (testing) 0.42085, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   385] loss: (training) 0.32483, (testing) 0.42069, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   386] loss: (training) 0.32483, (testing) 0.42083, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   387] loss: (training) 0.32482, (testing) 0.42096, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   388] loss: (training) 0.32480, (testing) 0.42073, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   389] loss: (training) 0.32481, (testing) 0.42027, accuracy: (training) 99.12109, (testing) 88.28125\n",
      "[epoch   390] loss: (training) 0.32480, (testing) 0.42003, accuracy: (training) 99.12109, (testing) 88.67188\n",
      "[epoch   391] loss: (training) 0.32479, (testing) 0.41991, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   392] loss: (training) 0.32478, (testing) 0.42000, accuracy: (training) 99.12109, (testing) 88.67188\n",
      "[epoch   393] loss: (training) 0.32475, (testing) 0.41968, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   394] loss: (training) 0.32479, (testing) 0.41990, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   395] loss: (training) 0.32474, (testing) 0.41977, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   396] loss: (training) 0.32474, (testing) 0.41920, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   397] loss: (training) 0.32471, (testing) 0.41894, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   398] loss: (training) 0.32473, (testing) 0.41864, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   399] loss: (training) 0.32473, (testing) 0.41815, accuracy: (training) 99.12109, (testing) 89.45312\n",
      "[epoch   400] loss: (training) 0.32467, (testing) 0.41758, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "############### CHANGE LEARNING_RATE 0.010 TO 0.005 ###############\n",
      "[epoch   401] loss: (training) 0.32465, (testing) 0.41732, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   402] loss: (training) 0.32465, (testing) 0.41711, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   403] loss: (training) 0.32463, (testing) 0.41699, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   404] loss: (training) 0.32461, (testing) 0.41680, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   405] loss: (training) 0.32363, (testing) 0.41665, accuracy: (training) 99.21875, (testing) 89.06250\n",
      "[epoch   406] loss: (training) 0.32460, (testing) 0.41656, accuracy: (training) 99.12109, (testing) 89.06250\n",
      "[epoch   407] loss: (training) 0.32460, (testing) 0.41638, accuracy: (training) 99.12109, (testing) 89.84375\n",
      "[epoch   408] loss: (training) 0.32458, (testing) 0.41632, accuracy: (training) 99.12109, (testing) 89.45312\n",
      "[epoch   409] loss: (training) 0.32455, (testing) 0.41634, accuracy: (training) 99.12109, (testing) 89.45312\n",
      "[epoch   410] loss: (training) 0.32454, (testing) 0.41613, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   411] loss: (training) 0.32455, (testing) 0.41594, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   412] loss: (training) 0.32454, (testing) 0.41573, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   413] loss: (training) 0.32453, (testing) 0.41560, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   414] loss: (training) 0.32450, (testing) 0.41541, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   415] loss: (training) 0.32450, (testing) 0.41520, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   416] loss: (training) 0.32447, (testing) 0.41513, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   417] loss: (training) 0.32447, (testing) 0.41500, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   418] loss: (training) 0.32444, (testing) 0.41487, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   419] loss: (training) 0.32443, (testing) 0.41480, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   420] loss: (training) 0.32442, (testing) 0.41471, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   421] loss: (training) 0.32441, (testing) 0.41469, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   422] loss: (training) 0.32440, (testing) 0.41457, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   423] loss: (training) 0.32441, (testing) 0.41458, accuracy: (training) 99.12109, (testing) 90.23438\n",
      "[epoch   424] loss: (training) 0.32439, (testing) 0.41436, accuracy: (training) 99.12109, (testing) 90.62500\n",
      "[epoch   425] loss: (training) 0.32439, (testing) 0.41436, accuracy: (training) 99.12109, (testing) 90.62500\n",
      "[epoch   426] loss: (training) 0.32436, (testing) 0.41432, accuracy: (training) 99.12109, (testing) 90.62500\n",
      "[epoch   427] loss: (training) 0.32435, (testing) 0.41421, accuracy: (training) 99.12109, (testing) 90.62500\n",
      "[epoch   428] loss: (training) 0.32432, (testing) 0.41406, accuracy: (training) 99.12109, (testing) 90.62500\n",
      "[epoch   429] loss: (training) 0.32434, (testing) 0.41402, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   430] loss: (training) 0.32432, (testing) 0.41401, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   431] loss: (training) 0.32429, (testing) 0.41388, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   432] loss: (training) 0.32428, (testing) 0.41382, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   433] loss: (training) 0.32426, (testing) 0.41375, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   434] loss: (training) 0.32429, (testing) 0.41365, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   435] loss: (training) 0.32425, (testing) 0.41358, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   436] loss: (training) 0.32423, (testing) 0.41361, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   437] loss: (training) 0.32424, (testing) 0.41354, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   438] loss: (training) 0.32423, (testing) 0.41351, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   439] loss: (training) 0.32423, (testing) 0.41339, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   440] loss: (training) 0.32422, (testing) 0.41339, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   441] loss: (training) 0.32422, (testing) 0.41339, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   442] loss: (training) 0.32417, (testing) 0.41332, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   443] loss: (training) 0.32411, (testing) 0.41322, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   444] loss: (training) 0.32420, (testing) 0.41321, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   445] loss: (training) 0.32417, (testing) 0.41312, accuracy: (training) 99.21875, (testing) 90.62500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch   446] loss: (training) 0.32415, (testing) 0.41320, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   447] loss: (training) 0.32417, (testing) 0.41305, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   448] loss: (training) 0.32412, (testing) 0.41301, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   449] loss: (training) 0.32411, (testing) 0.41302, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   450] loss: (training) 0.32411, (testing) 0.41297, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   451] loss: (training) 0.32411, (testing) 0.41301, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   452] loss: (training) 0.32407, (testing) 0.41298, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   453] loss: (training) 0.32406, (testing) 0.41276, accuracy: (training) 99.21875, (testing) 91.01562\n",
      "[epoch   454] loss: (training) 0.32407, (testing) 0.41271, accuracy: (training) 99.21875, (testing) 91.01562\n",
      "[epoch   455] loss: (training) 0.32406, (testing) 0.41274, accuracy: (training) 99.21875, (testing) 91.01562\n",
      "[epoch   456] loss: (training) 0.32405, (testing) 0.41273, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   457] loss: (training) 0.32400, (testing) 0.41270, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   458] loss: (training) 0.32401, (testing) 0.41268, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   459] loss: (training) 0.32399, (testing) 0.41263, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   460] loss: (training) 0.32401, (testing) 0.41269, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   461] loss: (training) 0.32399, (testing) 0.41264, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   462] loss: (training) 0.32398, (testing) 0.41270, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   463] loss: (training) 0.32397, (testing) 0.41266, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   464] loss: (training) 0.32397, (testing) 0.41259, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   465] loss: (training) 0.32395, (testing) 0.41263, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   466] loss: (training) 0.32393, (testing) 0.41257, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   467] loss: (training) 0.32392, (testing) 0.41255, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   468] loss: (training) 0.32391, (testing) 0.41245, accuracy: (training) 99.21875, (testing) 90.62500\n",
      "[epoch   469] loss: (training) 0.32392, (testing) 0.41247, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   470] loss: (training) 0.32391, (testing) 0.41246, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   471] loss: (training) 0.32389, (testing) 0.41247, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   472] loss: (training) 0.32387, (testing) 0.41243, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   473] loss: (training) 0.32387, (testing) 0.41248, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   474] loss: (training) 0.32384, (testing) 0.41249, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   475] loss: (training) 0.32386, (testing) 0.41251, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   476] loss: (training) 0.32384, (testing) 0.41242, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   477] loss: (training) 0.32383, (testing) 0.41234, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   478] loss: (training) 0.32382, (testing) 0.41229, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   479] loss: (training) 0.32379, (testing) 0.41224, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   480] loss: (training) 0.32380, (testing) 0.41221, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   481] loss: (training) 0.32378, (testing) 0.41224, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   482] loss: (training) 0.32378, (testing) 0.41216, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   483] loss: (training) 0.32374, (testing) 0.41224, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   484] loss: (training) 0.32376, (testing) 0.41215, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   485] loss: (training) 0.32375, (testing) 0.41219, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   486] loss: (training) 0.32375, (testing) 0.41214, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   487] loss: (training) 0.32373, (testing) 0.41210, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   488] loss: (training) 0.32370, (testing) 0.41208, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   489] loss: (training) 0.32368, (testing) 0.41207, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   490] loss: (training) 0.32369, (testing) 0.41203, accuracy: (training) 99.21875, (testing) 90.23438\n",
      "[epoch   491] loss: (training) 0.32368, (testing) 0.41195, accuracy: (training) 99.21875, (testing) 90.23438\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# iteration for the epoch\n",
    "# -----------------------------------------------------------------------------\n",
    "for e in trange(epoch):\n",
    "    \n",
    "    if e % 100 == 0 :\n",
    "        print(\"############### CHANGE LEARNING_RATE %0.3f TO %0.3f ###############\" % ((optimizer.param_groups[0]['lr']), lr_list[e // 200]))\n",
    "        optimizer.param_groups[0]['lr'] = lr_list[e // 200]\n",
    "    \n",
    "    result_train    = train()\n",
    "    result_test     = test()\n",
    "\n",
    "    loss_train_mean.append(result_train['loss_train_mean'])\n",
    "    loss_train_std.append(result_train['loss_train_std'])\n",
    "    loss_test.append(result_test['loss_test'])\n",
    "\n",
    "    accuracy_train.append(result_train['accuracy_train'])\n",
    "    accuracy_train_std.append(result_train['accuracy_train_std'])\n",
    "    accuracy_test.append(result_test['accuracy_test'])\n",
    "    \n",
    "    print(\"[epoch %5d] loss: (training) %0.5f, (testing) %0.5f, accuracy: (training) %0.5f, (testing) %0.5f\" % (e+1, result_train['loss_train_mean'], result_test['loss_test'],result_train['accuracy_train'], result_test['accuracy_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLOT TRAIN AND VALIDATION LOSS AT EVERY ITERATION\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "loss_std = np.array(loss_train_std)\n",
    "acc_std = np.array(accuracy_train_std)\n",
    "train_loss = np.array(loss_train_mean)\n",
    "train_accuracy = np.array(accuracy_train)\n",
    "\n",
    "fig, ax2 = plt.subplots(2,1,figsize=(20,12))\n",
    "ax2[0].set_title(\"Loss Value\")\n",
    "ax2[0].set_ylabel(\"Loss\", fontsize=\"12\")\n",
    "ax2[0].set_xlabel(\"Epoch\", fontsize=\"12\")\n",
    "ax2[0].plot(range(1, len(train_loss)+1), train_loss,'-r', label='Train Loss')\n",
    "ax2[0].plot(range(1, len(loss_test)+1), loss_test, '-b', label='Validation Loss')\n",
    "ax2[0].fill_between(range(1, len(train_loss)+1), train_loss+loss_std, train_loss-loss_std, facecolor='red')\n",
    "ax2[0].legend(fontsize=\"12\")\n",
    "\n",
    "# PLOT TRAIN AND VALIDATION ACCURACY AT EVERY ITERATION\n",
    "ax2[1].set_title(\"Accuracy\")\n",
    "ax2[1].set_ylabel(\"Accuracy\", fontsize=\"12\")\n",
    "ax2[1].set_xlabel(\"Epoch\", fontsize=\"12\")\n",
    "ax2[1].plot(range(1, len(train_accuracy)+1), train_accuracy, '-y', label='Train Accuracy')\n",
    "ax2[1].plot(range(1, len(accuracy_test)+1), accuracy_test, '-g', label='Validation Accuracy')\n",
    "ax2[1].fill_between(range(1, len(train_accuracy)+1), train_accuracy+acc_std, train_accuracy-acc_std, facecolor='y')\n",
    "ax2[1].legend(fontsize=\"12\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     " accuracy_test[-1] ": {},
     " accuracy_train[-1] ": {},
     " loss_test[-1] ": {},
     " loss_train_mean[-1] ": {}
    }
   },
   "source": [
    "| dataset | loss | accuracy |\n",
    "| :-- | -- | -- |\n",
    "| train | {{ loss_train_mean[-1] }} | {{ accuracy_train[-1] }} |\n",
    "| validation | {{ loss_test[-1] }} | {{ accuracy_test[-1] }} |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "806.989px",
    "left": "977.915px",
    "right": "20px",
    "top": "148.966px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
